% Template for the submission to:
% The Annals of Probability           [aop]
% The Annals of Applied Probability   [aap]
% The Annals of Statistics            [aos]
% The Annals of Applied Statistics    [aoas]
% Stochastic Systems                  [ssy]
%
% Author: In this template, the places where you need to add information
% (or delete line) are indicated by {???}.  Mostly the information
% required is obvious, but some explanations are given in lines starting
% Author:
% All other lines should be ignored.  After editing, there should be
% no instances of ??? after this line.


% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear

% \documentclass[onecolumn,journal]{IEEEtran}

\documentclass{article}
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides

\usepackage{listings}
\usepackage{amsmath,mathtools}
\usepackage{amsthm}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{caption,subcaption}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{multirow}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{multirow,bigstrut,threeparttable}
\usepackage{amsthm}
\usepackage{array}
\usepackage{bbm}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{latexsym}
% \usepackage{cite}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{epsfig}
\usepackage{psfrag}
\usepackage{setspace}
\usepackage{tikz-cd}

% \usepackage{hyperref}
\usepackage[%dvips,
CJKbookmarks=true,
bookmarksnumbered=true,
bookmarksopen=true,
% bookmarks=false,
colorlinks=true,
citecolor=red,
linkcolor=blue,
anchorcolor=red,
urlcolor=blue
]{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage[ruled]{algorithm2e}
% \usepackage{algpseudocode}
\usepackage{stfloats}
% \usepackage[backend=biber]{biblatex}
% \usepackage[backend=biber,style=nature]{biblatex}
\usepackage[
autocite    = superscript,
backend     = bibtex, % bibtex, biber. bibtex: warning: "Using fall-back BibTeX(8) backend:(biblatex) functionality may be reduced/unavailable."
sortcites   = true,
style       = authoryear % numeric; nature
]{biblatex} % to be deleted
\addbibresource{ref.bib}
% \usepackage[numbers]{natbib}
\usepackage{nameref}
% \usepackage{ulem} % overwrite \emph
\usepackage[normalem]{ulem}
\newcommand{\zg}[1]{{\color{blue} [ZG: #1]}}
\newcommand{\zgg}[2]{\xout{#1}{\color{blue}#2}}
\def\cluster{gene-trait cluster}

%% Added by QZ
\usepackage{soul}
\newcommand{\qz}[1]{{\color{red} [QZ: #1]}}
\newcommand{\qzz}[2]{\st{#1}{\color{red}#2}}

% provide arXiv number if available:
% \arxiv{cs.IT/1502.00326}
\input xy
\xyoption{all}

% \numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}[section]

\crefname{theorem}{Theorem}{Theorems}
\crefname{proposition}{Proposition}{Propositions}
\crefname{corollary}{Corollary}{Corollaries}


\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{question}{Question}
\newtheorem{prob}{Problem}
\newtheorem*{sol}{Solution}

\crefname{definition}{Definition}{Definitions}
\crefname{remark}{Remark}{Remarks}

% \setcounter{tocdepth}{1}

%%%%% page setup %%%%%

% \setlength{\textwidth}{460pt}
% \setlength{\oddsidemargin}{0pt}
% \setlength{\evensidemargin}{0pt}
% \setlength{\topmargin}{0pt}
% \setlength{\textheight}{620pt}

%%%%%%%%% mathbb %%%%%%%%

\def\AA{\mathbb{A}}
\def\BB{\mathbb{B}}
\def\CC{\mathbb{C}}
\def\DD{\mathbb{D}}
\def\EE{\mathbb{E}}
\def\FF{\mathbb{F}}
\def\GG{\mathbb{G}}
\def\HH{\mathbb{H}}
\def\II{\mathbb{I}}
\def\JJ{\mathbb{J}}
\def\KK{\mathbb{K}}
\def\LL{\mathbb{L}}
\def\MM{\mathbb{M}}
\def\NN{\mathbb{N}}
\def\OO{\mathbb{O}}
\def\PP{\mathbb{P}}
% \def\overline{\QQ}{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\RR{\mathbb{R}}
\def\SS{\mathbb{S}}
\def\TT{\mathbb{T}}
\def\UU{\mathbb{U}}
\def\VV{\mathbb{V}}
\def\WW{\mathbb{W}}
\def\XX{\mathbb{X}}
\def\YY{\mathbb{Y}}
\def\ZZ{\mathbb{Z}}

%%%%%%%% mathcal %%%%%%%%

\def\calA{\mathcal{A}}
\def\calB{\mathcal{B}}
\def\calC{\mathcal{C}}
\def\calD{\mathcal{D}}
\def\calE{\mathcal{E}}
\def\calF{\mathcal{F}}
\def\calG{\mathcal{G}}
\def\calH{\mathcal{H}}
\def\calI{\mathcal{I}}
\def\calJ{\mathcal{J}}
\def\calK{\mathcal{K}}
\def\calL{\mathcal{L}}
\def\calM{\mathcal{M}}
\def\calN{\mathcal{N}}
\def\calO{\mathcal{O}}
\def\calP{\mathcal{P}}
\def\calQ{\mathcal{Q}}
\def\calR{\mathcal{R}}
\def\calS{\mathcal{S}}
\def\calT{\mathcal{T}}
\def\calU{\mathcal{U}}
\def\calV{\mathcal{V}}
\def\calW{\mathcal{W}}
\def\calX{\mathcal{X}}
\def\calY{\mathcal{Y}}
\def\calZ{\mathcal{Z}}

%%%%%%%%% bold face %%%%%%%%%%

\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bC{\mathbf{C}}
\def\bD{\mathbf{D}}
\def\bE{\mathbf{E}}
\def\bF{\mathbf{F}}
\def\bG{\mathbf{G}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bJ{\mathbf{J}}
\def\bK{\mathbf{K}}
\def\bL{\mathbf{L}}
\def\bM{\mathbf{M}}
\def\bN{\mathbf{N}}
\def\bO{\mathbf{O}}
\def\bP{\mathbf{P}}
\def\bQ{\mathbf{Q}}
\def\bR{\mathbf{R}}
\def\bS{\mathbf{S}}
\def\bT{\mathbf{T}}
\def\bU{\mathbf{U}}
\def\bV{\mathbf{V}}
\def\bW{\mathbf{W}}
\def\bX{\mathbf{X}}
\def\bY{\mathbf{Y}}
\def\bZ{\mathbf{Z}}

%%%%%%%% frak %%%%%%%%

\newcommand\frA{\mathfrak{A}}
\newcommand\frB{\mathfrak{B}}
\newcommand\frg{\mathfrak{g}}
\newcommand\frt{\mathfrak{t}}
\newcommand\frb{\mathfrak{b}}
\newcommand\frh{\mathfrak{h}}
\newcommand\frn{\mathfrak{n}}
\newcommand\frl{\mathfrak{l}}
\newcommand\frp{\mathfrak{p}}
\def\frm{\mathfrak{m}}



%%%%%%%% tilde %%%%%%%%%


\newcommand\tilW{\widetilde{W}}
\newcommand\tilA{\widetilde{A}}
\newcommand\tilB{\widetilde{B}}
\newcommand\tilC{\widetilde{C}}
\newcommand\tilD{\widetilde{D}}
\newcommand\tilE{\widetilde{E}}
\newcommand\tilF{\widetilde{F}}
\newcommand\tilG{\widetilde{G}}



\newcommand\alg{\textup{alg}}
\newcommand\Alg{\textup{Alg}}
\newcommand\Aut{\textup{Aut}}
\newcommand{\codim}{\textup{codim}}
\newcommand{\coker}{\textup{coker}}
\newcommand\ev{\textup{ev}}
\newcommand\Fr{\textup{Fr}}
\newcommand\Frob{\textup{Frob}}
\newcommand\Gal{\textup{Gal}}
\newcommand{\Gr}{\textup{Gr}}
\newcommand{\gr}{\textup{gr}}
\newcommand\Hom{\textup{Hom}}
\newcommand\id{\textup{id}}
\renewcommand{\Im}{\textup{Im}}
\newcommand{\Ind}{\textup{Ind}}
\newcommand{\ind}{\textup{ind}}
\newcommand\Isom{\textup{Isom}}
\newcommand\Lie{\textup{Lie}}
\newcommand\Mat{\textup{Mat}}
\newcommand\Mod{\textup{Mod}}
\newcommand{\Nm}{\textup{Nm}}
\newcommand\pr{\textup{pr}}
\newcommand\rank{\textup{rank}}
\newcommand{\red}{\textup{red}}
\newcommand\Rep{\textup{Rep}}
\newcommand{\Res}{\textup{Res}}
\newcommand\res{\textup{res}}
\newcommand\rk{\textup{rk}}
\newcommand\Span{\textup{Span}}
\newcommand\Spec{\textup{Spec}\ }
\newcommand\St{\textup{St}}
% \newcommand\st{\textup{st}} % deleted by QZ
\newcommand\Stab{\textup{Stab}}
\newcommand\Sym{\textup{Sym}}
\newcommand{\Tr}{\textup{Tr}}
\newcommand{\tr}{\textup{tr}}



\newcommand\bij{\leftrightarrow}
\newcommand{\incl}{\hookrightarrow}
\newcommand{\isom}{\stackrel{\sim}{\to}}
\newcommand{\leftexp}[2]{{\vphantom{#2}}^{#1}{#2}}
\newcommand{\trpose[1]}{\leftexp{t}{#1}}
\newcommand{\jiao}[1]{\langle{#1}\rangle}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\un}[1]{\underline{#1}}
\newcommand\chk{\textup{char}(k)}
\newcommand\kbar{\overline{k}}
\newcommand\ep{\epsilon}
\renewcommand\l{\lambda}
\newcommand\height{\textup{ht}}
\newcommand\leng{\textup{leng}}
\newcommand\diag{\textup{diag}}

\def\frp{\mathfrak{p}}
\def\pto{\to_p}%{\overset{p}{\longrightarrow}}
\def\dto{\to_d}
\def\1{\mathbbm{1}}
\def\var{\mathsf{Var}}
\def\cov{\mathsf{Cov}}
\def\cor{\mathsf{Cor}}
% \def\st{\mathsf{st}} %deleted by QZ
\def\nd{\mathsf{nd}}
\def\rd{\mathsf{rd}}
\def\th{\mathsf{th}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
% \def\argmax{\mathsf{argmax}}
% \def\argmin{\mathsf{argmin}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

% tables
\usepackage{booktabs}
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{listings}



% opening

% put your definitions there:

% \newtheorem{remark}{Remark} \def\remref#1{Remark~\ref{#1}}
% \newtheorem{conjecture}{Conjecture} \def\remref#1{Remark~\ref{#1}}
% \newtheorem{example}{Example}

% \theorembodyfont{\itshape}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{lemma}{Lemma} \def\lemref#1{Lemma~\ref{#1}}
% \newtheorem{corollary}{Corollary}


% \theorembodyfont{\rmfamily}
% \newtheorem{definition}{Definition}
% \numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{conjecture}{Conjecture}
\newtheorem*{induction}{Induction Hypothesis}

\def \by {\bar{y}}
\def \bx {\bar{x}}
\def \bh {\bar{h}}
\def \bz {\bar{z}}
\def \cF {\mathcal{F}}
\def \bP {\mathbb{P}}
% \def \bE {\mathbb{E}}
\def \bR {\mathbb{R}}
\def \bF {\mathbb{F}}
\def \cG {\mathcal{G}}
\def \cM {\mathcal{M}}
\def \cB {\mathcal{B}}
\def \cN {\mathcal{N}}
\def \var {\mathsf{Var}}


















%%%%%%%%%%%%%%%%%%%%%%%%%%%% by Wu %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xspace}

\newcommand{\Lip}{\mathrm{Lip}}
\newcommand{\stepa}[1]{\overset{\rm (a)}{#1}}
\newcommand{\stepb}[1]{\overset{\rm (b)}{#1}}
\newcommand{\stepc}[1]{\overset{\rm (c)}{#1}}
\newcommand{\stepd}[1]{\overset{\rm (d)}{#1}}
\newcommand{\stepe}[1]{\overset{\rm (e)}{#1}}


\newcommand{\floor}[1]{{\left\lfloor {#1} \right \rfloor}}
\newcommand{\ceil}[1]{{\left\lceil {#1} \right \rceil}}

\newcommand{\blambda}{\bar{\lambda}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Prob}{\mathbb{P}}
% \newcommand{\overline{\QQ}}[1]{\mathbb{P}[#1]}
\newcommand{\intd}{{\rm d}}
\newcommand{\TV}{{\sf TV}}
\newcommand{\LC}{{\sf LC}}
\newcommand{\PW}{{\sf PW}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\eexp}{{\rm e}}
\newcommand{\expects}[2]{\mathbb{E}_{#2}\left[ #1 \right]}
\newcommand{\diff}{{\rm d}}
\newcommand{\eg}{e.g.\xspace}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\iid}{i.i.d.\xspace}
\newcommand{\fracp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\fracpk}[3]{\frac{\partial^{#3} #1}{\partial #2^{#3}}}
\newcommand{\fracd}[2]{\frac{\diff #1}{\diff #2}}
\newcommand{\fracdk}[3]{\frac{\diff^{#3} #1}{\diff #2^{#3}}}
\newcommand{\renyi}{R\'enyi\xspace}
\newcommand{\lpnorm}[1]{\left\|{#1} \right\|_{p}}
\newcommand{\linf}[1]{\left\|{#1} \right\|_{\infty}}
\newcommand{\lnorm}[2]{\left\|{#1} \right\|_{{#2}}}
\newcommand{\Lploc}[1]{L^{#1}_{\rm loc}}
\newcommand{\hellinger}{d_{\rm H}}
\newcommand{\Fnorm}[1]{\lnorm{#1}{\rm F}}
%% parenthesis
\newcommand{\pth}[1]{\left( #1 \right)}
\newcommand{\qth}[1]{\left[ #1 \right]}
\newcommand{\sth}[1]{\left\{ #1 \right\}}
\newcommand{\bpth}[1]{\Bigg( #1 \Bigg)}
\newcommand{\bqth}[1]{\Bigg[ #1 \Bigg]}
\newcommand{\bsth}[1]{\Bigg\{ #1 \Bigg\}}
\newcommand{\xxx}{\textbf{xxx}\xspace}
\newcommand{\toprob}{{\xrightarrow{\Prob}}}
\newcommand{\tolp}[1]{{\xrightarrow{L^{#1}}}}
\newcommand{\toas}{{\xrightarrow{{\rm a.s.}}}}
\newcommand{\toae}{{\xrightarrow{{\rm a.e.}}}}
\newcommand{\todistr}{{\xrightarrow{{\rm D}}}}
\newcommand{\eqdistr}{{\stackrel{\rm D}{=}}}
\newcommand{\iiddistr}{{\stackrel{\text{\iid}}{\sim}}}
% \newcommand{\var}{\mathsf{var}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Poi}{\mathsf{Poi}}
\newcommand{\iprod}[2]{\left \langle #1, #2 \right\rangle}
\newcommand{\Iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\indc}[1]{{\mathbf{1}_{\left\{{#1}\right\}}}}
\newcommand{\Indc}{\mathbf{1}}

\definecolor{myblue}{rgb}{.8, .8, 1}
\definecolor{mathblue}{rgb}{0.2472, 0.24, 0.6} % mathematica's Color[1, 1--3]
\definecolor{mathred}{rgb}{0.6, 0.24, 0.442893}
\definecolor{mathyellow}{rgb}{0.6, 0.547014, 0.24}


\newcommand{\blue}{\color{blue}}
\newcommand{\nb}[1]{{\sf\blue[#1]}}
\newcommand{\nbr}[1]{{\sf\red[#1]}}

\newcommand{\tmu}{{\tilde{\mu}}}
\newcommand{\tf}{{\tilde{f}}}
\newcommand{\tp}{\tilde{p}}
\newcommand{\tilh}{{\tilde{h}}}
\newcommand{\tu}{{\tilde{u}}}
\newcommand{\tx}{{\tilde{x}}}
\newcommand{\ty}{{\tilde{y}}}
\newcommand{\tz}{{\tilde{z}}}
\newcommand{\tA}{{\tilde{A}}}
\newcommand{\tB}{{\tilde{B}}}
\newcommand{\tC}{{\tilde{C}}}
\newcommand{\tD}{{\tilde{D}}}
\newcommand{\tE}{{\tilde{E}}}
\newcommand{\tF}{{\tilde{F}}}
\newcommand{\tG}{{\tilde{G}}}
\newcommand{\tH}{{\tilde{H}}}
\newcommand{\tI}{{\tilde{I}}}
\newcommand{\tJ}{{\tilde{J}}}
\newcommand{\tK}{{\tilde{K}}}
\newcommand{\tL}{{\tilde{L}}}
\newcommand{\tM}{{\tilde{M}}}
\newcommand{\tN}{{\tilde{N}}}
\newcommand{\tO}{{\tilde{O}}}
\newcommand{\tP}{{\tilde{P}}}
\newcommand{\tQ}{{\tilde{Q}}}
\newcommand{\tR}{{\tilde{R}}}
\newcommand{\tS}{{\tilde{S}}}
\newcommand{\tT}{{\tilde{T}}}
\newcommand{\tU}{{\tilde{U}}}
\newcommand{\tV}{{\tilde{V}}}
\newcommand{\tW}{{\tilde{W}}}
\newcommand{\tX}{{\tilde{X}}}
\newcommand{\tY}{{\tilde{Y}}}
\newcommand{\tZ}{{\tilde{Z}}}

\newcommand{\sfa}{{\mathsf{a}}}
\newcommand{\sfb}{{\mathsf{b}}}
\newcommand{\sfc}{{\mathsf{c}}}
\newcommand{\sfd}{{\mathsf{d}}}
\newcommand{\sfe}{{\mathsf{e}}}
\newcommand{\sff}{{\mathsf{f}}}
\newcommand{\sfg}{{\mathsf{g}}}
\newcommand{\sfh}{{\mathsf{h}}}
\newcommand{\sfi}{{\mathsf{i}}}
\newcommand{\sfj}{{\mathsf{j}}}
\newcommand{\sfk}{{\mathsf{k}}}
\newcommand{\sfl}{{\mathsf{l}}}
\newcommand{\sfm}{{\mathsf{m}}}
\newcommand{\sfn}{{\mathsf{n}}}
\newcommand{\sfo}{{\mathsf{o}}}
\newcommand{\sfp}{{\mathsf{p}}}
\newcommand{\sfq}{{\mathsf{q}}}
\newcommand{\sfr}{{\mathsf{r}}}
\newcommand{\sfs}{{\mathsf{s}}}
\newcommand{\sft}{{\mathsf{t}}}
\newcommand{\sfu}{{\mathsf{u}}}
\newcommand{\sfv}{{\mathsf{v}}}
\newcommand{\sfw}{{\mathsf{w}}}
\newcommand{\sfx}{{\mathsf{x}}}
\newcommand{\sfy}{{\mathsf{y}}}
\newcommand{\sfz}{{\mathsf{z}}}
\newcommand{\sfA}{{\mathsf{A}}}
\newcommand{\sfB}{{\mathsf{B}}}
\newcommand{\sfC}{{\mathsf{C}}}
\newcommand{\sfD}{{\mathsf{D}}}
\newcommand{\sfE}{{\mathsf{E}}}
\newcommand{\sfF}{{\mathsf{F}}}
\newcommand{\sfG}{{\mathsf{G}}}
\newcommand{\sfH}{{\mathsf{H}}}
\newcommand{\sfI}{{\mathsf{I}}}
\newcommand{\sfJ}{{\mathsf{J}}}
\newcommand{\sfK}{{\mathsf{K}}}
\newcommand{\sfL}{{\mathsf{L}}}
\newcommand{\sfM}{{\mathsf{M}}}
\newcommand{\sfN}{{\mathsf{N}}}
\newcommand{\sfO}{{\mathsf{O}}}
\newcommand{\sfP}{{\mathsf{P}}}
\newcommand{\sfQ}{{\mathsf{Q}}}
\newcommand{\sfR}{{\mathsf{R}}}
\newcommand{\sfS}{{\mathsf{S}}}
\newcommand{\sfT}{{\mathsf{T}}}
\newcommand{\sfU}{{\mathsf{U}}}
\newcommand{\sfV}{{\mathsf{V}}}
\newcommand{\sfW}{{\mathsf{W}}}
\newcommand{\sfX}{{\mathsf{X}}}
\newcommand{\sfY}{{\mathsf{Y}}}
\newcommand{\sfZ}{{\mathsf{Z}}}


\newcommand{\bara}{{\bar{a}}}
\newcommand{\barb}{{\bar{b}}}
\newcommand{\barc}{{\bar{c}}}
\newcommand{\bard}{{\bar{d}}}
\newcommand{\bare}{{\bar{e}}}
\newcommand{\barf}{{\bar{f}}}
\newcommand{\barg}{{\bar{g}}}
\newcommand{\barh}{{\bar{h}}}
\newcommand{\bari}{{\bar{i}}}
\newcommand{\barj}{{\bar{j}}}
\newcommand{\bark}{{\bar{k}}}
\newcommand{\barl}{{\bar{l}}}
\newcommand{\barm}{{\bar{m}}}
\newcommand{\barn}{{\bar{n}}}
\newcommand{\baro}{{\bar{o}}}
\newcommand{\barp}{{\bar{p}}}
\newcommand{\barq}{{\bar{q}}}
\newcommand{\barr}{{\bar{r}}}
\newcommand{\bars}{{\bar{s}}}
\newcommand{\bart}{{\bar{t}}}
\newcommand{\baru}{{\bar{u}}}
\newcommand{\barv}{{\bar{v}}}
\newcommand{\barw}{{\bar{w}}}
\newcommand{\barx}{{\bar{x}}}
\newcommand{\bary}{{\bar{y}}}
\newcommand{\barz}{{\bar{z}}}
\newcommand{\barA}{{\bar{A}}}
\newcommand{\barB}{{\bar{B}}}
\newcommand{\barC}{{\bar{C}}}
\newcommand{\barD}{{\bar{D}}}
\newcommand{\barE}{{\bar{E}}}
\newcommand{\barF}{{\bar{F}}}
\newcommand{\barG}{{\bar{G}}}
\newcommand{\barH}{{\bar{H}}}
\newcommand{\barI}{{\bar{I}}}
\newcommand{\barJ}{{\bar{J}}}
\newcommand{\barK}{{\bar{K}}}
\newcommand{\barL}{{\bar{L}}}
\newcommand{\barM}{{\bar{M}}}
\newcommand{\barN}{{\bar{N}}}
\newcommand{\barO}{{\bar{O}}}
\newcommand{\barP}{{\bar{P}}}
\newcommand{\barQ}{{\bar{Q}}}
\newcommand{\barR}{{\bar{R}}}
\newcommand{\barS}{{\bar{S}}}
\newcommand{\barT}{{\bar{T}}}
\newcommand{\barU}{{\bar{U}}}
\newcommand{\barV}{{\bar{V}}}
\newcommand{\barW}{{\bar{W}}}
\newcommand{\barX}{{\bar{X}}}
\newcommand{\barY}{{\bar{Y}}}
\newcommand{\barZ}{{\bar{Z}}}

\newcommand{\hX}{\hat{X}}
\newcommand{\Ent}{\mathsf{Ent}}

\newcommand{\trans}{^{\rm T}}
\newcommand{\Th}{{^{\rm th}}}
\newcommand{\diverge}{\to \infty}
\newcommand{\testConst}{\pi}

%% nc BH
\usepackage{enumitem}

\newcommand{\No}{{n}}
\newcommand{\NoNull}{{n_0}}
\newcommand{\NoMinus}{{n^-}}
\newcommand{\NoPlus}{{n^+}}
\newcommand{\NoZero}{{n^0}}
\newcommand{\NoNonNull}{{n_1}}
% \newcommand{\NoNc}{{n_{\text{nc}}}}
\newcommand{\NoNc}{m}
\newcommand{\probNull}{{\pi_0}}
% \newcommand{\ncProbNull}{{\pi^{\text{nc}}}}

\newcommand{\pval}[1]{{p_{#1}}}
\newcommand{\pvalOrder}[1]{{p_{(#1)}}}
\newcommand{\pvalAll}{{\boldsymbol{p}}}
\newcommand{\qval}[1]{{p_{#1}}}
\newcommand{\qvalOrder}[1]{{p_{(#1)}}}
\newcommand{\qvalAll}{{\boldsymbol{p}}}
\newcommand{\testStatistics}[1]{{T_{#1}}}
\newcommand{\testStatisticsOrder}[1]{{T_{(#1)}}}
\newcommand{\testStatisticsAll}{{\boldsymbol{T}}}
\newcommand{\testStatisticsAllTwo}{{\boldsymbol{T}^{\calH}}}
\newcommand{\cdfTestStatistics}[1]{{F_{#1}}}
\newcommand{\cdfTestStatisticsNull}{{F_{0}}}
\newcommand{\cdfTestStatisticsNonNull}{{F_{1}}}
\newcommand{\cdfTestStatisticsOrder}[1]{{F_{(#1)}}}
\newcommand{\pdfTestStatistics}[1]{{f_{#1}}}
\newcommand{\pdfTestStatisticsNull}{{f_{0}}}
\newcommand{\pdfTestStatisticsNonNull}{{f_{1}}}
\newcommand{\pdfTestStatisticsOrder}[1]{{f_{(#1)}}}

% \newcommand{\ncPval}[1]{{p^{\text{nc}}_{#1}}}
% \newcommand{\ncPvalOrder}[1]{{p^{\text{nc}}_{(#1)}}}
% \newcommand{\ncQval}[1]{{p^{\text{nc}}_{#1}}}
% \newcommand{\ncQvalOrder}[1]{{p^{\text{nc}}_{(#1)}}}
% \newcommand{\ncQvalAll}{{\boldsymbol{p}^{\text{nc}}}}
% \newcommand{\ncTestStatistics}[1]{{T^{\text{nc}}_{#1}}}
% \newcommand{\ncTestStatisticsOrder}[1]{{T^{\text{nc}}_{(#1)}}}
% \newcommand{\ncTestStatisticsAll}{{\boldsymbol{T}^{\text{nc}}}}
% \newcommand{\ncCdfTestStatistics}[1]{{F^{\text{nc}}_{#1}}}
% \newcommand{\ncCdfTestStatisticsOrder}[1]{{F^{\text{nc}}_{(#1)}}}
% \newcommand{\ncPdfTestStatistics}[1]{{f^{\text{nc}}_{#1}}}
% \newcommand{\ncPdfTestStatisticsOrder}[1]{{f^{\text{nc}}_{(#1)}}}

% \newcommand{\ncPval}[1]{{p^{\text{nc}}_{#1}}}
% \newcommand{\ncPvalOrder}[1]{{p^{\text{nc}}_{(#1)}}}
% \newcommand{\ncQval}[1]{{p^{\text{nc}}_{#1}}}
% \newcommand{\ncQvalOrder}[1]{{p^{\text{nc}}_{(#1)}}}
% \newcommand{\ncQvalAll}{{\boldsymbol{p}^{\text{nc}}}}
\newcommand{\ncTestStatistics}[1]{{C_{#1}}}
\newcommand{\ncTestStatisticsOrder}[1]{{C_{(#1)}}}
% \newcommand{\ncTestStatisticsAll}{{\boldsymbol{N}}}
\newcommand{\ncTestStatisticsAll}{{\boldsymbol{C}}} % ZJ
\newcommand{\ncCdfTestStatistics}[1]{{F^{\text{nc}}_{#1}}}
\newcommand{\ncCdfTestStatisticsOrder}[1]{{F^{\text{nc}}_{(#1)}}}
\newcommand{\ncPdfTestStatistics}[1]{{f^{\text{nc}}_{#1}}}
\newcommand{\ncPdfTestStatisticsOrder}[1]{{f^{\text{nc}}_{(#1)}}}

\newcommand{\nickname}{{\text{RANC}}}
\newcommand{\nicknameTwo}{{\text{DiRANC}}}


\newcommand{\hypothesis}[1]{{H_{#1}}}
\newcommand{\hypothesisOrder}[1]{{H_{(#1)}}}
\newcommand{\hypothesisSet}{{\calH}}
\newcommand{\hypothesisIndex}[1]{{\calI_{#1}}}
\newcommand{\nullHypothesis}[1]{{H_{0,#1}}}
\newcommand{\nullHypothesisOrder}[1]{{H_{0,(#1)}}}
\newcommand{\nullHypothesisSet}{{\calH_{0}}}
\newcommand{\nullHypothesisIndex}{{\calI_{0}}}
\newcommand{\alternativeHypothesis}[1]{{H_{1,#1}}}
\newcommand{\alternativeHypothesisOrder}[1]{{H_{1,(#1)}}}
\newcommand{\nonNullHypothesisSet}{{\calH_{1}}}
\newcommand{\nonNullHypothesisIndex}{{\calI_{1}}}
\newcommand{\ncHypothesis}[1]{{H^{\text{nc}}_{#1}}}
\newcommand{\ncHypothesisOrder}[1]{{H^{\text{nc}}_{(#1)}}}
\newcommand{\ncHypothesisSet}{{\calH_{\text{nc}}}}
\newcommand{\ncNullHypothesis}[1]{{H^{\text{nc}}_{0,#1}}}
\newcommand{\ncNullHypothesisOrder}[1]{{H^{\text{nc}}_{0,(#1)}}}
\newcommand{\ncAlternativeHypothesis}[1]{{H^{\text{nc}}_{1,#1}}}
\newcommand{\ncAlternativeHypothesisOrder}[1]{{H^{\text{nc}}_{1,(#1)}}}

\newcommand{\responseTreatment}[1]{{Y_{#1}^{\text{t}}}}
\newcommand{\responseControl}[1]{{Y_{#1}^{\text{c}}}}

\newcommand{\truePositive}{{S}}
\newcommand{\falsePositive}{{V}}
\newcommand{\ncFalsePositive}{{V_{\text{nc}}}}
\newcommand{\trueNegative}{{U}}
\newcommand{\ncTrueNegative}{{U_{\text{nc}}}}
\newcommand{\falseNegative}{{T}}
\newcommand{\totalPositive}{{R}}

\newcommand{\globalNull}{{H}}
\newcommand{\FWER}{\text{FWER}}
\newcommand{\kFWER}{{k\text{-FWER}}}
\newcommand{\FWERDir}{\text{FWER}^{\text{dir}}}
\newcommand{\FDR}{\text{FDR}}
\newcommand{\mFDR}{\text{mFDR}}
\newcommand{\FDRDir}{\text{FDR}^{\text{dir}}}
\newcommand{\FDRLevel}{q}
\newcommand{\FDP}{\text{FDP}}
\newcommand{\FDPDir}{\text{FDP}^{\text{dir}}}
\newcommand{\localFDR}{\text{local-FDR}}
\newcommand{\localFDRDir}{\text{local-FDR}^{\text{dir}}}
\newcommand{\LocalFDR}{\text{Local-FDR}}
\newcommand{\LocalFDRDir}{\text{Local-FDR}^{\text{dir}}}

\newcommand{\PRDS}{{\text{PRDS}}}
\newcommand{\MTPTwo}{{$\text{MTP}_2$}}
\newcommand{\BH}{{\text{BH}}}

\newcommand{\logTMT}{{$\log$ TMT}}

\newcommand{\cutoff}{c}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\LRT}{\text{LRT}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\bt}{\boldsymbol{t}}

\newcommand{\permutationFunction}[1]{{G(#1)}}
\newcommand{\fullRank}[1]{{R#1}}
\newcommand{\partialRank}[1]{{r#1}}
\newcommand{\symmetricGroup}[1]{{\text{Sym}(#1)}}

\newcommand{\stoppingTime}{{\tau_{0,q}}}
\newcommand{\stoppingTimeLambda}{{\tau_{0,q}}}
\newcommand{\stoppingTimeSecond}{{\tau_{q,2}}}
\newcommand{\stoppingTimeSecondLambda}{{\tau_{q,2}^\lambda}}

%%% new local FDR and Bayes risk
\newcommand{\estimatedLocalFDR}[1]{{\hat{q}(#1)}}
\newcommand{\threshold}{{t}}
\newcommand{\trueThreshold}{{\tau^*_\lambda}}
\newcommand{\trueThresholdMinimizer}[1]{{\tau^*_{\lambda, \min, #1}}}
\newcommand{\trueThresholdMaximizer}[1]{{\tau^*_{\lambda, \max, #1}}}
\newcommand{\estimatedThreshold}{{\hat{\tau}_{\lambda, \No}}}
\newcommand{\estimatedThresholdMinimizer}[1]{{\hat{\tau}_{\lambda, \No, \min, #1}}}
\newcommand{\estimatedThresholdMaximizer}[1]{{\hat{\tau}_{\lambda, \No, \max, #1}}}
\newcommand{\EmpiricalThreshold}{{\hat{\tau}_{\lambda, \No, \NoNc}}}
\newcommand{\EmpiricalThresholdMinimizer}[1]{{\hat{\tau}_{\lambda, \No, \NoNc, \min, #1}}}
\newcommand{\EmpiricalThresholdMaximizer}[1]{{\hat{\tau}_{\lambda, \No, \NoNc, \max, #1}}}
\newcommand{\EmpiricalThresholdSecond}{{\hat{\tau}_{\lambda, \No}}}
\newcommand{\EmpiricalThresholdSecondMinimizer}[1]{{\hat{\tau}_{\lambda, \No, \min, #1}}}
\newcommand{\EmpiricalThresholdSecondMaximizer}[1]{{\hat{\tau}_{\lambda, \No, \max, #1}}}
\newcommand{\BayesRisk}{{R}}
\newcommand{\BayesRiskLambda}{{R_{\lambda}}}
\newcommand{\EmpiricalBayesRisk}{{{\ell}_{\No, \NoNc}}}
\newcommand{\EmpiricalBayesRiskLambda}{{{\ell}_{\lambda, \No, \NoNc}}}
\newcommand{\EmpiricalBayesRiskLambdaSecond}{{{\ell}_{\lambda, \No}}}
\newcommand{\EmpiricalCdfTestStatistics}{{{F}_n}}
\newcommand{\EmpiricalCdfTestStatisticsNull}{{{F}_{0,\NoNc}}}
\newcommand{\EmpiricalCdfTestStatisticsNullSecond}{{{F}_{0,\No}}}
\newcommand{\EmpiricalPdfTestStatistics}{{{f}_n}}
\newcommand{\EmpiricalPdfTestStatisticsNull}{{{f}_{0,\NoNc}}}
\newcommand{\EmpiricalPdfTestStatisticsNullSecond}{{{f}_{0,\No}}}
\newcommand{\pdfBased}{{\text{PDF-based}}}
\newcommand{\PdfBased}{{\text{PDF-based}}}
\newcommand{\cdfBased}{{\text{CDF-based}}}
\newcommand{\CdfBased}{{\text{CDF-based}}}
\newcommand{\estimatedHypothesis}[1]{{\hat{H}_{#1}}}
\newcommand{\optimalEstimatedHypothesis}[1]{{\hat{H}_{#1}^{\text{opt}}}}
\newcommand{\weightNull}{{w_0}}
\newcommand{\weightNonNull}{{w_1}}
\newcommand{\level}{{q}}
\newcommand{\EmpiricalProbNull}{{\hat{\pi}}}
\newcommand{\monotoneTransformation}[1]{{h(#1)}}
\newcommand{\weight}[1]{{w_{#1}}}
\newcommand{\window}[1]{{h_{#1}}}
\newcommand{\lowerBound}[1]{{L_{#1}}}
\newcommand{\upperBound}[1]{{U_{#1}}}
\newcommand{\interval}[1]{{I_{#1}}}


\newcommand{\pvalPlus}[1]{{p_{#1}^+}}
\newcommand{\pvalMinus}[1]{{p_{#1}^-}}
\newcommand{\sign}{{\text{sign}}}
\newcommand{\signShort}[1]{{S_{#1}}}

% directional RANC
\newcommand{\selection}{\text{selection method}}
\newcommand{\Selection}{\text{Selection method}}
\newcommand{\twoWay}{\text{two-way method}}
\newcommand{\TwoWay}{\text{Two-way method}}

% HTE estimator assessment
\newcommand{\train}{\text{tr}}

% optimal transport
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}


% influence function
\newcommand{\IFC}{\text{IF}}
\newcommand{\EIFC}{\text{EIF}}


\title{Trustworthy assessment of heterogeneous treatment effect estimator via analysis of relative error}
\author{
% Tobias Freidling\textsuperscript{1} \qquad 
% Qingyuan Zhao\textsuperscript{1}\qquad 
Zijun Gao
% {\small \textsuperscript{1}Statistical Laboratory, University of Cambridge, UK}\\
  \footnote{\small Marshall School of Business, University of Southern California, USA}
}

\begin{document}

\maketitle

% TODO:
% 0. Proofs
% 1. Simulations
% 2. Compare >=3 HTE estimators.
% 3. Future directions

\begin{abstract}
    % We develop a powerful and robust method for comparing heterogeneous treatment effect estimators based on the semi-parametrically efficient estimator of relative errors.
    Accurate heterogeneous treatment effect (HTE) estimation is essential for personalized recommendations, making it important to evaluate and compare HTE estimators. Traditional assessment methods are inapplicable due to missing counterfactuals.
    Current HTE evaluation methods rely on additional estimation or matching on test data, often ignoring the uncertainty introduced and potentially leading to incorrect conclusions. 
    We propose incorporating uncertainty quantification into HTE estimator comparisons.
    In addition, we suggest shifting the focus to the estimation and inference of the relative error between methods rather than their absolute errors.
    Methodology-wise, we develop a relative error estimator based on the efficient influence function and establish its asymptotic distribution for inference. Compared to absolute error-based methods, the relative error estimator (1) is less sensitive to the error of nuisance function estimators, satisfying a "global double robustness" property, and (2) its confidence intervals are often narrower, making it more powerful for determining the more accurate HTE estimator. 
    Through extensive empirical study of the ACIC challenge benchmark datasets, we show that the relative error-based method more effectively identifies the better HTE estimator with statistical confidence, even with a moderately large test dataset or inaccurate nuisance estimators.
\end{abstract}


\section{Introduction}\label{sec:introduction}

The estimation of heterogeneous treatment effects (HTE) under the Neyman-Rubin potential outcome framework is becoming increasingly prominent, driven by the need for tailored approaches in areas such as personalized medicine, personalized education, and personalized advertising. 
A variety of machine learning tools are being employed to estimate HTE, including LASSO, random forests, gradient boosting, and neural networks. 
Despite a rich body of research on HTE estimation, evaluating and comparing these estimators remain less investigated.
% There are two primary reasons to focus on the assessment of HTE estimators. 
% Firstly, assessing an estimator's absolute accuracy predicts its performance on future data. 
% \zg{Goal: determine the better one.}
% \zg{Directly estimating the better one, and in particular, construct CI for that. Estimator is a simply difference, but CI is not...}
Evaluating the performance of HTE estimators is essential for identifying a better candidate, especially considering the wide range of HTE estimation methods available.


% including the degree of penalization in LASSO, tree sizes in random forests, number of layers in neural networks. 
One significant challenge in assessing an HTE estimator arises from the inherent missingness in potential outcome model.
Provided with a test dataset, standard evaluation of a predictor compares the actual observations with the predicted values.
% This is feasible because the observations are unbiased samples of the predicted values. 
However, in the potential outcome model, each observed response corresponds to one potential outcome (treatment or control), and the HTE---the difference between two potential outcomes---is not directly observed.
To deal with the missingness of HTE, existing methods of HTE assessment typically involves additional steps performed on the test dataset, such as matching \parencite{rolling2014model} or nuisance function estimation \parencite{alaa2019validating}, to construct pseudo-observations of the HTE (\Cref{sec:literature}). 
The additional steps could introduce substantial randomness, which may even dominate the actual error difference between two HTE estimators.
% where the randomness of the error exceeds the magnitude of the gap of two HTE estimators.
Simply ignoring this source of randomness and outputting the estimator with the lower point error estimate could result in incorrect decisions with a significant probability.
% This observation motivates us to account for the uncertainty of the estimated errors in model comparison. 


% Among existing methods of HTE assessment, one popular method considers the prediction of the observed outcomes \zg{Ref}.
% Accurate predictions of both the treated and the control potential outcomes are sufficient for a precise estimation of the treatment effect, but not necessary.
% In addition, there are approaches, like various \zg{tree-based methods of HTE}, yielding only the estimates of HTE but not the potential outcomes, and the outcome-prediction-based assessment method can not be applied.
% Another line of methods first match treated and control units to create ``virtual twins'', and use the difference between the paired units' observed outcomes as pseudo-observations of the HTE \zg{Ref}. Nevertheless, the matching can be computationally intensive, and less viable when there is a significant imbalance (leaving part of the units unmatched and thus not used) between the treatment arm and the control arm. 

% \subsection{Proposal overview and contributions}\label{sec:overview}

In this paper, we advocate that the comparison of HTE estimators should account for the randomness introduced during the evaluation stage. 
Rather than providing a point estimate of the error, we suggest constructing a confidence interval for the evaluation error, which contains the true error value with a pre-specified probability.
We then demonstrate that the confidence interval for the absolute error of an HTE estimator (1) can be sensitive to nuisance function estimation on the test data; (2) for two similar HTE estimators, their absolute error confidence intervals do not account for the similarity of the HTE estimators and could be too wide to determine the more accurate estimator.
To address the issues of uncertainty quantification for absolute errors, we propose to directly construct confidence intervals for the \textit{relative} error between two HTE estimators, rather than their individual absolute errors. 
Methodologically, we derive an efficient estimator for the relative error using influence functions and characterize its asymptotic distribution to facilitate the confidence interval construction. 
Theoretically, we prove that our confidence interval of relative errors is valid under weaker assumptions regarding the quality of the nuisance function estimators compared to that of absolute errors, and is guaranteed to be narrow when the HTE estimators for comparison are similar.
Empirically, we show that the relative error confidence intervals achieves better coverage as well as are more powerful in identifying the better HTE estimator.
Beyond the difference in conditional means, our proposal can also be generalized to compare estimators of a broader class of heterogeneous treatment effect estimands more suitable for quantifying treatment effects for non-continuous outcomes.


% \noindent\textbf{Contributions}. 
% Our contributions are three-fold:
% \begin{itemize}
%     \item [1.] For comparing HTE estimators, we propose to account for the uncertainty in estimating the error of HTE estimators, which is largely ignored in the literature. Taking the uncertainty into consideration yields more trustworthy conclusion about the quality comparison of HTE estimators.
%     \item [2.] We suggest constructing confidence intervals of the relative error between two HTE estimators rather than their absolute errors. 
%     We propose a one-step correction estimator and the associated confidence interval for the relative error based on the efficient influence function, and prove the asymptotic validity and optimality of the proposed confidence interval. 
%     We prove the relative error estimator is less sensitive to nuisance function estimators, enjoys a global doubly robustness property, and is useful in selecting the better HTE estimator.
% \end{itemize}
% On the simulated data, we observe the constructed interval of the relative error achieves the desirable coverage corresponding to controlling the probability of making incorrect conclusions.

\noindent\textbf{Organization}.
The paper is organized as follows. 
In \Cref{sec:background}, we present the problem formulation, introducing the potential outcome model, specifying the causal estimand, defining the absolute and relative errors for HTE estimators, and providing a review of the relevant literature. In \Cref{sec:absolute.error}, we provide the efficient estimator, confidence interval of absolute errors, and discuss the issues therein.
In \Cref{sec:relative.error}, we focus on relative errors, presenting the efficient estimator and its associated confidence interval, and explain how the issues with absolute error confidence intervals are avoided.
We also discuss the generalization of our proposal based on relative error to a broader class of causal estimands. 
In \Cref{sec:simulation}, we compare the confidence intervals for absolute and relative errors on a benchmark dataset from the 2016 ACIC challenge. 
In \Cref{sec:discussion}, we conclude with a summary and outline future research directions. 
All proofs are provided in the supplementary materials.

% \zg{Add figures of uncertainty quantification to the main text.}


\section{Formulation and background}\label{sec:background}

\subsection{Potential outcome model}\label{sec:potential.outcome.model}

We follow the Neyman-Rubin potential outcome model with a treatment condition and a control condition.
% Suppose there are $n$ units.
For unit $i$, there is a $d$-dimensional covariate vector $X_i$, a binary treatment assignment $W_i \in \{0, 1\}$, and two potential outcomes: $Y_i(0)$ under the control condition and $Y_i(1)$ under the treatment condition.
We denote the observed outcome by $Y_i$, which equals $Y_i(1)$ if the unit is under treatment, and $Y_i(0)$, otherwise.
We use $Z_i = (X_i, W_i, Y_i)$ to denote all the observed data of unit $i$.


We make the conventional assumptions of SUTVA, overlap, and unconfoundedness.
\begin{assumption}[Stable unit treatment value assumption (SUTVA)]
    The potential outcomes for any unit do not depend on the treatments assigned to other units. There are no different versions of each treatment level.
\end{assumption}

\begin{assumption}[Unconfoundedness]
    The assignment mechanism does not depend on potential outcomes:
    \begin{align*}
        (Y_i(1), Y_i(0)) \perp W_i \mid X_i.
    \end{align*}
\end{assumption}

\begin{assumption}[Overlap]\label{assu:overlap}
    There is a positive probability of receiving treatment and control for all individuals.
\end{assumption}


To define our causal estimand, we introduce the super-population. 
We assume individuals are sampled i.i.d. from a super-population, denoted by $\PP$.
In particular, the covariates $X_i$ are sampled from an unknown distribution $\PP_X$. 
Given the covariates $X_i$, a binary group assignment $W_i \in \{0, 1\}$ is generated from a Bernoulli distribution with mean $e(X_i)$ (also known as the propensity score). 
Echoing with \Cref{assu:overlap}, we assume there exists $\eta > 0$  such that $\eta < e(x) < 1 - \eta$.
The potential outcomes are modeled by
\begin{align*}
    Y_i(1) | X_i &= \mu_1(X_i) + \epsilon_i, \\
    Y_i(0) | X_i &= \mu_0(X_i) + \epsilon_i,  
\end{align*}
where $\mu_0(x)$, $\mu_1(x)$ represent the conditional mean function of the control and the treatment group respectively, and $\epsilon_i$ denotes the error term, assumed to be zero-mean and independent of $X_i$. 
The estimand HTE is defined as 
\begin{align*}
    \tau(x) = \EE\left[Y(1) - Y(0) \mid X = x\right] 
    = \mu_1(x) - \mu_0(x),
\end{align*}
which can also be expressed as the difference of the two group conditional mean functions.



\subsection{Absolute error and relative error}\label{sec:error.definition}

In this paper, we focus on the evaluation and comparison of the HTE estimators using a test dataset of size $n$ drawn from the super-population $\PP$.
We use $\hat{\tau}_1(x)$, $\hat{\tau}_2(x)$ to denote HTE estimators derived independently of the test dataset.
When there is only one HTE estimator, we drop the subscript and use $\hat{\tau}(x)$.
We highlight that the HTE estimators are provided to us and the HTE estimation problem itself is not the focus of this paper.


To quantify the accuracy of an HTE estimator $\hat{\tau}(x)$, the absolute error is defined as
\begin{align}\label{defi:evaluation.error}
    \phi(\hat{\tau}(x))
    := \EE\left[(\hat{\tau}(X) - \tau(X))^2\right],
\end{align}
where the expectation is evaluated at the distribution $\PP_X$ of the covariates. 
A smaller absolute error suggests a more accurate HTE estimator, and a zero error implies $\hat{\tau}(X) = {\tau}(X)$ with probability one.
% Eq.~\eqref{defi:evaluation.error}  is a direct extension of the prediction error commonly used in regression.
The relative error of two estimators $\hat{\tau}_1(x)$ and $\hat{\tau}_2(x)$ is quantified as the difference between their absolute errors 
\begin{align}\label{defi:evaluation.error.relative}
    \delta(\hat{\tau}_1, \hat{\tau}_2)
    := \phi(\hat{\tau}_1(x)) - \phi(\hat{\tau}_2(x))  
    = \EE\left[\hat{\tau}_1^2(X) - \hat{\tau}_2^2(X) - 2(\hat{\tau}_1(X) - \hat{\tau}_2(X)) \tau(X) \right].
\end{align}  
A negative $\delta(\hat{\tau}_1, \hat{\tau}_2)$ indicates that $\hat{\tau}_1(x)$ is more accurate; otherwise, $\hat{\tau}_2(x)$ is more accurate.
In \Cref{rmk:DINA}, we consider the treatment effects defined on the natural parameter scale suitable for binary, count, and survival responses, and extend the results regarding Eq.~\eqref{defi:evaluation.error} to the corresponding errors.


For standard predictor evaluation, absolute prediction errors are more commonly displayed than relative prediction errors.
However, for HTE estimators, we have the perhaps surprising observation that the relative error can often be approximated more accurately.
% and associated with a narrower confidence interval than the absolute counterpart. 
Intuitively, this is because the relative error $ \delta(\hat{\tau}_1, \hat{\tau}_2)$ is linear in the unobserved $\tau(x)$, while the absolute error $\phi(\hat{\tau}(x))$ also depends on the second moment of $\tau(x)$, and estimating the first moment of $\tau(x)$ is relatively easier that of the second moment. 
We provide rigorous characterizations and empirical comparison of the observation in the following sections.



\subsection{Literature of HTE evaluation}\label{sec:literature}

In this paper, we consider comparing HTE estimators with statistical confidence through relative error evaluation, which is different from the mainstream literature focusing on evaluating absolute errors without uncertainty quantification. 
Nevertheless, we provide a brief overview of existing methods on assessing the absolute performance of an HTE estimator.
One simple and common approach targets the observed response but not the treatment effect, and uses the standard prediction error of the response as the error measurement.
However, an accurate predictor of the treatment effect may not necessarily be associated with precise response predictors \parencite{curth2023search}.
Furthermore, for HTE estimators that directly estimate the difference without estimating the response, such as causal trees \parencite{athey2016recursive}, the response prediction error can not be computed.
Another thread of assessment approaches involves creating ``virtual twins'' by matching treated and control units and use the response difference of a pair as a pseudo-observation of the treatment effect \parencite{rolling2014model}.
% zijun gao
However, matching is often computationally intensive \parencite{rosenbaum1989optimal}. Moreover, the complexity of matching algorithms makes it difficult to analyze statistically and perform downstream inference.
A third thread of methods estimate the HTE on the test dataset and compare it with the provided HTE estimators, which we refer to as plug-in estimators. This can be problematic because the evaluation error is affected by the error from the HTE estimator obtained from the test set, a nuisance function in this causal assessment task.
To reduce the impact of the error of the HTE estimator from the test data, bias correction methods based on influence functions have been developed \parencite{alaa2019validating}. 
The method is related to ours, but it does not address the uncertainty quantification of the estimated error.
Additionally, the influence function in \parencite{alaa2019validating} is different from our efficient proposal (comparison can be found in \Cref{sec:simulation}).


Our approach for drawing inference on relative error evaluation builds on the rapidly advancing body of work in semi-parametrics, particularly influence functions \parencite{van2000asymptotic, robins2008higher} (see \cite{kennedy2022semiparametric} for a review).  
In many causal problems, the causal quantity of interest is typically a scalar or low-dimensional, but the model contains infinite dimensional nuisance functions, making it a semi-parametric problem.
The influence function is a powerful tool for constructing the so-called one-step correction estimators that is more robust to the error of the estimators of nuisance components.
Given a specific function class that the true distribution belongs to, the estimator that attains the minimal asymptotic variance is known as the semi-parametrically efficient estimator, and the corresponding influence function is referred to as the efficient influence function. Despite the appealing statistical properties of efficient influence functions, the derivation of efficient influence functions is often case-specific, with only a few general rules and standard examples available \parencite{kennedy2022semiparametric}.


\section{Absolute error and issues}\label{sec:absolute.error}

In this section, we begin by discussing the estimation and inference of the absolute error using influence functions. We then highlight the undesirable properties of the absolute error estimation, which leads to our proposal of using relative error in \Cref{sec:relative.error}.


\subsection{Absolute error estimation via influence functions}\label{sec:absolute.error.estimation}
% Though XXX has developed a one-step correction estimator for $ \phi(\hat{\tau})$ in~\eqref{defi:evaluation.error} using the influence function in (XXX, Theorem 2), 
Provided with an HTE estimator $\hat{\tau}(x)$,
we adopt the following influence function for $\phi(\hat{\tau})$, 
\begin{align}\label{eq:EIF}
\begin{split}    
    \psi(\phi(\hat{\tau}); Z)
    :=&  \left((\mu_1(X) - \mu_0(X)) - \hat{\tau}(X)\right)^2 \\
    &+ 2\left((\mu_1(X) - \mu_0(X)) - \hat{\tau}(X)\right) \cdot \left(\frac{W(Y - \mu_1(X))}{e(X)} - \frac{(1-W)(Y - \mu_0(X))}{1-e(X)} \right) - \phi(\hat{\tau}).
\end{split}
\end{align}
Our derivation of the efficient influence function aligns with the variable importance measurement for heterogeneous treatment effects \parencite{hines2022variable}.
% We use the influence function for common causal quantities, such as the weighted average treatment effect, and apply the principles of influence function derivation for a linear combination of causal estimands and that for a function of a causal estimand.
The one-step correction estimator associated with \eqref{eq:EIF} is
\begin{align}\label{eq:estimator.absolute.error}
\begin{split}    
    \hat{\phi}(\hat{\tau})
    :=& \frac{1}{n} \sum_{i=1}^n \hat{\psi}(\phi(\hat{\tau}); Z_i)
    = \frac{1}{n} \sum_{i=1}^n \left((\tilde{\mu}_1(X_i) - \tilde{\mu}_0(X_i)) - \hat{\tau}(X_i)\right)^2  \\
    &+ 2\left((\tilde{\mu}_1(X_i) - \tilde{\mu}_0(X_i)) - \hat{\tau}(X_i)\right) \cdot \left(\frac{W_i(Y_i - \tilde{\mu}_1(X_i))}{\tilde{e}(X_i)} - \frac{(1-W_i)(Y_i - \tilde{\mu}_0(X_i))}{1-\tilde{e}(X_i)} \right).
\end{split}
\end{align}
Here $\tilde{\mu}_0(x)$, $\tilde{\mu}_1(x)$, and $\tilde{e}(x)$ are estimators of the nuisance functions ${\mu}_0(x)$, ${\mu}_1(x)$, and $e(x)$ obtained on the test dataset\footnote{We use cross-fitting \parencite{chernozhukov2018double} to ensure the independence of $\tilde{\mu}_0(x)$, $\tilde{\mu}_1(x)$, and $\tilde{e}(x)$ used by $\hat{\phi}_i(\hat{\tau})$ are independent of $Y_i$, $W_i$, $X_i$ therein when computing $\hat{\phi}(\hat{\tau})$.}.
The variance of the estimated evaluation error, denoted by ${V}(\hat{\phi}(\hat{\tau}))$, can be approximated by the empirical variance of $\hat{\psi}(\phi(\hat{\tau}); Z_i)$, denoted by $\hat{V}(\hat{\phi}(\hat{\tau}))$.
The $1-\alpha$ confidence interval of the absolute error takes the form 
\begin{align}\label{eq:CI.absolute.error}
    [\underline{\hat{\phi}}(\hat{\tau}; 1-\alpha), ~\overline{\hat{\phi}}(\hat{\tau}; 1-\alpha)]:= \left[\hat{\phi}(\hat{\tau}) - q_{1-\alpha/2} \hat{V}^{1/2}(\hat{\phi}(\hat{\tau})), ~\hat{\phi}(\hat{\tau}) + q_{1-\alpha/2} \hat{V}^{1/2}(\hat{\phi}(\hat{\tau}))\right],
\end{align} 
where $q_{1-\alpha/2}$ denotes the $1-\alpha/2$ quantile of a standard normal random variable.
The entire algorithm is summarized in \Cref{algo:absolute.error}.
% When provided with two HTE estimators $\hat{\tau}_1(x)$, $\hat{\tau}_2(x)$, we construct the two confidence intervals... 


The theorem below characterize the asymptotic distribution of $\hat{\phi}(\hat{\tau})$, which provides the theoretical guarantee of the validity of the confidence interval~\eqref{eq:CI.absolute.error}.
\begin{theorem}\label{theo:absolute.error}
    Assume the following conditions.
    \begin{itemize}
        \item [(a)] $Y$ is bounded, $\eta < e(X) < 1 - \eta$ for some $\eta > 0$.
        \item [(b)] The nuisance function estimators $\tilde{\mu}_{0}(x)$, $\tilde{\mu}_{1}(x)$, $\tilde{e}(x)$ obtained from the test data\footnote{When cross-fitting is used to compute the absolute error, we require (b) to hold for all $\tilde{\mu}_{0}^{-k}(x)$, $\tilde{\mu}_{1}^{-k}(x)$, $\tilde{e}^{-k}(x)$, $1 \le k \le K$.} satisfy $\EE[(\tilde{\mu}_{1}(X) - \mu_1(X))^2]^{1/2}$, $\EE[(\tilde{\mu}_{0}(X) - \mu_0(X))^2]^{1/2}$, $\EE[(\hat{e}(X) - e(X))^2]^{1/2} = o_p(n^{-1/4})$, $1 \le k \le K$. 
        \item [(c)] The true absolute error $\phi(\hat{\tau}) > 0$.
    \end{itemize}
    Then $\hat{\phi}(\hat{\tau})$, $\hat{V} (\hat{\phi}(\hat{\tau}))$ of \Cref{algo:absolute.error} satisfy
    \begin{align*}
        \frac{\hat{\phi}(\hat{\tau}) - {\phi}(\hat{\tau})}{\sqrt{n\hat{V}(\hat{\phi}(\hat{\tau}))}}
        \stackrel{d}{\to} \calN(0,1).
    \end{align*}
    In addition, the estimator $\hat{\phi}(\hat{\tau})$ is semi-parametrically efficient regarding the nonparametric model.
\end{theorem}


The proof is provided in the appendix.
% In \Cref{sec:relative.error}, we show the doubly robust property of the relative error described in is even more compelling.
We remark that our influence function and the estimator is different from that of \cite[Theorem 2]{alaa2019validating}.
Since our proposal $\hat{\phi}(\hat{\tau})$ is semi-parametrically efficient, the confidence interval is proved to be no wider than that in \cite{alaa2019validating} (see \Cref{sec:simulation} for numerical evidence).
% as it only requires the product of $|(\tilde{\mu}_{1}^{-k}(X) - \mu_1(X))(\hat{e}^{-k}(X) - e(X))|$ and $ |(\tilde{\mu}_{0}^{-k}(X) - \mu_0(X))(\hat{e}^{-k}(X) - e(X))| $ to be of order $ n^{-1/2}$.
% and is less sensitive to the error in $\tilde{\mu}_{0}^{-k}(X)$ and $\tilde{\mu}_{1}^{-k}(X)$.
% both $\tilde{\mu}_{0}^{-k}(X)$ and $\tilde{\mu}_{1}^{-k}(X)$ underfit, leading to $\tilde{\mu}_{1}^{-k}(X) - \tilde{\mu}_{0}^{-k}(X) \approx 0$. 
% This result in a significant downward bias in the absolute error estimator. 
% In contrast, the relative error estimator remains nearly unbiased since $\hat{e}^{-k}(X)$ is accurate and $(\tilde{\mu}_{0}^{-k}(X) - \mu_0(X))(\hat{e}^{-k}(X) - e(X))$ is small in magnitude.
According to \Cref{theo:absolute.error}, the convergence of $\hat{\phi}(\hat{\tau})$ at the parametric rate of $ n^{-1/2} $ only requires all nuisance function estimators to converge at a rate no slower than $ n^{-1/4}$, that is the locally doubly robust property \parencite{chernozhukov2018double}.
% meaning that $\hat{\phi}(\hat{\tau})$ is resilient to the errors of the nuisance function estimators around their true values.


Based on the $1-\alpha$ confidence interval of the absolute errors for two HTE estimators $\hat{\tau}_1$, $\hat{\tau}_2$, if the absolute error interval of $\hat{\tau}_1$ lies entirely to the right of that of $\hat{\tau}_2$, we can conclude with at least $1-2\alpha$ confidence that the estimation error of $\hat{\tau}_1$ is greater than that of $\hat{\tau}_2$, and therefore $\hat{\tau}_2$ should be selected. 
If the two intervals overlap, we are unable to confidently decide which estimator is more accurate.


\subsection{Issues of absolute error}\label{sec:absolute.error.issue}

Despite the results in \Cref{theo:absolute.error}, we highlight several undesirable aspects of the absolute error approach. In \Cref{sec:relative.error} below, we demonstrate how relative error estimation can address these issues.
\begin{itemize}
    \item [(i)] Sensitivity to errors of nuisance function estimators. 
    In \Cref{fig:inaccurate nuisance function estimator}, errors in $\tilde{\mu}_1(x)$, $\tilde{\mu}_0(x)$, $\tilde{e}(x)$ can introduce significant bias into the absolute error estimator, even resulting in negative estimates conflicting with the fact that error should always be non-negative. 
    % Rather than requiring $\tilde{\mu}_0(x)$, $\tilde{\mu}_1(x)$, and $\tilde{e}(x)$ to be estimated at the rate $n^{-1/4}$, respectively, it would be preferable if the condition can be relaxed to only the products of the errors $(\tilde{\mu}_w(x) - \mu_w(x)) (\tilde{e}(x) - e(x))$, for $w \in \{0,1\}$, are required to converge at the rate $n^{-1/2}$.
    
    \item [(ii)] Correlation across the estimated absolute errors of different HTE estimators.
    The estimated absolute error of different HTE estimators are correlated because they are based on the same validation data and share the complex nuisance function estimators.
    \Cref{theo:absolute.error} guides the construction of confidence intervals for each estimator separately, but does not directly address the correlation between the estimated absolute errors.

    \item [(iii)] Degenerate null. Condition (c) of \Cref{theo:absolute.error} indicates the asymptotic distribution may be invalid for the degenerate case $\PP_X(\hat{\tau}(X) = \tau(X)) = 1$. 
    One solution is analyzing the asymptotic distribution of the higher-order pathwise derivatives of $\phi(\hat{\tau})$ to derive the asymptotic distribution of \eqref{eq:estimator.absolute.error} when $\tau(x) = \hat{\tau}(x)$, which remains generally an open problem \parencite{hines2022variable, hudson2023nonparametric}.
    % Similar degeneracy issues have been encountered in the study of variable importance \parencite{hines2022variable}, where their estimand is expressed as the difference between two quantities and a sample splitting procedure can be employed albeit with some loss of efficiency. However, in our case, there does not appear to be a straightforward way to decompose the quantity in a similar manner.
    % As illustrated later in \Cref{sec:relative.error}, the degeneracy is less an issue for the relative error.

\end{itemize}


\begin{figure}[h]
        \centering
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/inaccurate_nuisance_function_estimator_absolute_LASSO.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/inaccurate_nuisance_function_estimator_absolute_xgboost.pdf}
        \end{minipage}
        \hspace{1cm}
            \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/inaccurate_nuisance_function_estimator_relative.pdf}
        \end{minipage}
        \subcaption*{\hspace{3cm} Absolute error \hspace{5cm} Relative error}
        \caption{Comparison of estimated absolute and relative errors with inaccurate nuisance function estimators.
        We compare two HTE estimators: one uses LASSO for nuisance function estimation, and the other uses gradient boosting. 
        For both HTE estimators, we implement three approaches to evaluate the absolute and the relative errors: (1) the plug-in method, (2) the estimator by \cite{alaa2019validating} (IF), and (3) our proposal (EIF). 
        (Our proposal relative error estimator agrees with that based on the plug-in absolute error estimator, and thus we merge the two methods in the third panel.)
        In all methods, cross-fitting is used and the nuisance function estimators are shared. 
        All evaluation methods use the same nuisance function estimators derived from the test dataset, which were intentionally designed to underfit the data and thus inaccurate.
        % The propensity score is estimated by taking the mean of the treatment assignments. The outcome regression functions $ \mu_0(x) $ and $ \mu_1(x) $ are both estimated using gradient boosting to the outcomes of the control and treatment group data. 
        % However, to introduce deliberate inaccuracies in the nuisance functions, we limit the number of boosting rounds, leading to underfitting in the estimators for $ \mu_0(x) $ and $ \mu_1(x) $.
        For each evaluation, we plot the estimated error as well as the $90\%$ confidence interval.
        The red dashed line in the figure represents the true absolute/relative error.
        The IF and EIF estimators for absolute error are negative, which conflicts with the non-negative nature of prediction errors. 
        Only the confidence interval of the EIF relative error correctly captures the true value.
        }
    \label{fig:inaccurate nuisance function estimator}
\end{figure}


% We explain the reasons why the asymptotic distribution of $\phi(\hat{\tau})$ in \Cref{algo:absolute.error} is violated if $\hat{\tau}(x) = \tau(x)$.
% Explicitly, $\hat{\phi}(\hat{\tau})$ satisfies
% \begin{align}\label{eq:IF.expansion}  
%     \hat{\phi}(\hat{\tau})
%     = {\phi}(\hat{\tau}) + \frac{1}{n} \sum_{i=1}^n \left( \hat{\phi}_i(\hat{\tau}) - {\phi}(\hat{\tau}) \right) + R_n, \quad R_n =  o_p(n^{-1/2}).
% \end{align}
% When $\phi(\hat{\tau}) = 0$, under the property (b) of \Cref{theo:absolute.error}, $|(\tilde{\mu}_{1}^{-k}(x) - \tilde{\mu}_{0}^{-k}(x)) - \hat{\tau}(x)|$ is of order $n^{-1/4}$, and
% \begin{align*}
%     &\frac{1}{n} \sum_{i=1}^n \left((\tilde{\mu}_1(X) - \tilde{\mu}_0(X)) - \hat{\tau}(X)\right)^2 = o_p(n^{-1/2}), \\
%     &\frac{1}{n} \sum_{i=1}^n \left((\tilde{\mu}_1(X) - \tilde{\mu}_0(X)) - \hat{\tau}(X)\right) \cdot \left(\frac{W(Y - \tilde{\mu}_1(X))}{\tilde{e}(X)} - \frac{(1-W)(Y - \tilde{\mu}_0(X))}{1-\tilde{e}(X)} \right) = o_p(n^{-1/2}),
% \end{align*}
% which could be of the same order of the remainder term $R_n$. As a result, the asymptotic distribution in \Cref{theo:absolute.error} derived by ignoring $R_n$ is not valid.
% As illustrated later in \Cref{sec:relative.error}, the degeneracy is less an issue for the relative error.

% We discuss several solutions in dealing with  $\phi(\hat{\tau}) = 0$.
% One possibility is testing $\tau(x) = \hat{\tau}(x)$ and apply \Cref{theo:absolute.error} only if the null hypothesis is rejected.
% Testing an HTE function is challenging and of independent interest, and we do not pursue this further here.
% Alternatively, one can analyze the asymptotic distribution of the higher-order pathwise derivatives of $\phi(\hat{\tau})$ and derive the asymptotic distribution of \eqref{eq:estimator.absolute.error} when $\tau(x) = \hat{\tau}(x)$. 
% However, this remains generally an open problem {Carone2018, Hudson2023}.
% Similar degeneracy issues have been encountered in the study of variable importance, where the target is expressed as the difference between two quantities. In that context, the influence function of each individual quantity remains non-degenerate, even if the influence function of their difference is degenerate, allowing for the application of a sample splitting procedure, albeit with some loss of efficiency. However, in our case, there does not appear to be a straightforward way to decompose the quantity in a similar manner.
% % Moreover, in our case, if $\hat{\tau}(x)$ is different from $\tau(x)$ but close to $\hat{\tau}^{-k}(x) = \tilde{\mu}_{1}^{-k}(x) - \tilde{\mu}_{0}^{-k}(x)$, perhaps due to over-regularization applied to both $\hat{\tau}(x)$ and $\hat{\tau}^{-k}(x)$, then the CI will be too narrow and the CI invalid. This is also referred to as \zg{homopoly in XXX}.




\section{Relative error estimation and inference}\label{sec:relative.error}


In this section, we focus on the estimation and inference of relative error based on influence functions. We then compare the relative error estimation with the absolute counterpart regarding the issues in \Cref{sec:absolute.error.issue}.


\subsection{Relative error estimation via influence functions}\label{sec:relative.error.estimation}

Provided with two HTE estimators $\hat{\tau}_1(x)$, $\hat{\tau}_2(x)$, we adopt the following influence function for the relative error $\delta(\hat{\tau}_1, \hat{\tau}_2)$,
% Based on \Cref{eq:EIF} and the principle that the influence function of two causal quantities is the difference of the their influence functions , we arrive at
\begin{align}\label{eq:EIF.relative} 
\begin{split}    
    &\psi(\delta(\hat{\tau}_1, \hat{\tau}_2); Z)
    := \hat{\tau}_1^2(X) - \hat{\tau}_2^2(X) \\
    &- 2\left(\hat{\tau}_1(X) - \hat{\tau}_2(X)\right) \cdot \left(\frac{W(Y - \mu_1(X))}{e(X)} + \mu_1(X)- \frac{(1-W)(Y - \mu_0(X))}{1-e(X)} - \mu_0(X)\right) - \delta(\hat{\tau}_1, \hat{\tau}_2).
\end{split}
\end{align}
The implied one-step correction estimator is
\begin{align}\label{eq:estimator.relative.error}
\begin{split}    
    \hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)
    :=&\frac{1}{n} \sum_{i=1}^n  \hat{\psi}(\delta(\hat{\tau}_1, \hat{\tau}_2); Z_i)
    = \frac{1}{n} \sum_{i=1}^n \hat{\tau}_1^2(X_i) - \hat{\tau}_2^2(X_i) \\
    &- 2\left(\hat{\tau}_1(X_i) - \hat{\tau}_2(X_i)\right) \cdot \left(\frac{W_i(Y_i - \tilde{\mu}_1(X_i))}{\tilde{e}(X_i)} + \tilde{\mu}_1(X_i)- \frac{(1-W_i)(Y_i - \tilde{\mu}_0(X_i))}{1-\tilde{e}(X_i)} - \tilde{\mu}_0(X_i)\right).
\end{split}
\end{align}
The algorithm can be summarized as \Cref{algo:absolute.error} combined with \eqref{eq:estimator.relative.error}.


Based on the $1-\alpha$ confidence interval of the relative error, if the interval lies entirely to the right of zero, we can conclude with at least $1-\alpha$ confidence that the estimation error of $\hat{\tau}_1$ is greater than that of $\hat{\tau}_2$, and therefore $\hat{\tau}_2$ should be selected. 
If the interval lies entirely to the left of zero, we can conclude with the same confidence that the estimation error of $\hat{\tau}_2$ is greater than that of $\hat{\tau}_1$, and $\hat{\tau}_1$ should be chosen. 
If the interval contains zero, we are unable to confidently decide which estimator is superior.


\begin{algorithm}\caption{Absolute (relative) error}\label{algo:absolute.error}
    \begin{algorithmic}[1]
    \STATE \textbf{Input}: An HTE estimator $\hat{\tau}(x)$, test data $Z_i = (X_i, W_i, Y_i)$, $1 \le i \le n$, methods of estimating nuisance functions $\mu_0(x)$, $\mu_1(x)$, $e(x)$, number of folds $K$ for cross-fitting, confidence level $1-\alpha$.
    \STATE Randomly split the test dataset into $K$ folds of approximately equal size. Denote the $k$-th fold by $D_k$.

    \FOR{$k = 1,\ldots,K$}
    
    \STATE Apply the nuisance function estimators to test folds $\cup_{j \neq k} D_j$ and obtain $\tilde{\mu}_{0}^{-k}(x)$, $\tilde{\mu}_{1}^{-k}(x)$, and $\tilde{e}^{-k}(x)$.

    \ENDFOR
    
    \STATE Compute the one-step correction estimator based on \Cref{eq:estimator.absolute.error} (\Cref{eq:estimator.relative.error}). 
    For $i \in D_k$,
    \begin{align*} 
        \hat{\psi}_i^+(\phi(\hat{\tau}); Z_i) :=& \left((\tilde{\mu}_1^{-k}(X_i) - \tilde{\mu}_0^{-k}(X_i)) - \hat{\tau}(X_i)\right)^2 \\
        &+ 2\left((\tilde{\mu}_1^{-k}(X_i) - \tilde{\mu}_0^{-k}(X_i)) - \hat{\tau}(X_i)\right) \cdot \left(\frac{W_i(Y_i - \tilde{\mu}_1^{-k}(X_i))}{\tilde{e}^{-k}(X_i)} - \frac{(1-W_i)(Y_i - \tilde{\mu}_0^{-k}(X_i))}{1-\tilde{e}^{-k}(X_i)} \right),
    \end{align*}
     and take the average
     \begin{align*}
        \hat{\phi}(\hat{\tau}) :=& \frac{1}{n} \sum_{k=1}^K \sum_{i \in D_k} \hat{\psi}_i^+(\phi(\hat{\tau}); Z_i).
    \end{align*}
    Here $\hat{\psi}_i^+(\phi(\hat{\tau}); Z_i) $ and $\hat{\psi}_i(\phi(\hat{\tau}); Z_i)$ differ in a constant.
    
    \STATE Compute the estimator of the variance of $\hat{\phi}(\hat{\tau})$,
    \begin{align*}
       \hat{V}(\hat{\phi}(\hat{\tau})) :=& \frac{1}{n} \sum_{i = 1}^n \left(\hat{\psi}_i^+(\phi(\hat{\tau}); Z_i)  -  \hat{\phi}(\hat{\tau}) \right)^2.
    \end{align*}
    
    \STATE \textbf{Output}: Estimated error $\hat{\phi}(\hat{\tau})$, the estimator of its variance $\hat{V}(\hat{\phi}(\hat{\tau}))$, $1-\alpha$ confidence interval $\left[\hat{\phi}(\hat{\tau}) - q_{1-\alpha/2} \hat{V}^{1/2}(\hat{\phi}(\hat{\tau})), ~\hat{\phi}(\hat{\tau}) + q_{1-\alpha/2} \hat{V}^{1/2}(\hat{\phi}(\hat{\tau}))\right]$.

    \end{algorithmic}
\end{algorithm}


Similar to \Cref{theo:absolute.error}, we characterize the asymptotic distribution of $\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)$.
The proof is provided in the appendix.
\begin{theorem}\label{theo:relative.error}
    Assume the following conditions.
    \begin{itemize}
        \item [(a)] $Y$ is bounded, $\eta< e(X) < 1 - \eta$ for some $\eta > 0$.
        \item [(b)] The nuisance function estimators obtained from the test data satisfy $\|\tilde{\mu}_{1}(X) - \mu_1(X)\|_2$, $\|\tilde{\mu}_{0}(X) - \mu_0(X)\|_2$, $\|\hat{e}(X) - e(X)\|_2 = o_p(1)$, and $\EE[|(\tilde{\mu}_{1}(X) - \mu_1(X))(\hat{e}(X) - e(X))|]$, $\EE[|(\tilde{\mu}_{0}(X) - \mu_0(X))(\hat{e}(X) - e(X))|] = o_p(n^{-1/2})$.
        \item [(c)] The true relative error $\EE[(\hat{\tau}_1(X) - \hat{\tau}_2(X))^2] \neq 0$.
    \end{itemize}
    Then $\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)$, $\hat{V}(\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2))$ of \Cref{algo:absolute.error} satisfy
    \begin{align*}
        \frac{ \hat{\delta}(\hat{\tau}_1, \hat{\tau}_2) -     {\delta}(\hat{\tau}_1, \hat{\tau}_2)}{\sqrt{n\hat{V}(    \hat{\delta}(\hat{\tau}_1, \hat{\tau}_2))}}
        \stackrel{d}{\to} \calN(0,1).
    \end{align*}
    The estimator $\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)$ is efficient regarding the nonparametric model.

    Further assume
      \begin{itemize}
        \item [(d)] $\hat{e}(x) = e(x)$ or $\tilde{\mu}_{1}(x) = \mu_1(x)$, $\tilde{\mu}_{0}(x) = \mu_0(x)$.
    \end{itemize}
    Then the estimator $\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)$ is unbiased, i.e., $\EE\left[\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)\right]
        = {\delta}(\hat{\tau}_1, \hat{\tau}_2)$.
\end{theorem}



\subsection{Advantages of relative error}\label{sec:relative.error.advantage}

We compare \Cref{theo:relative.error} and \Cref{theo:absolute.error}, and discuss how the estimation and inference of relative errors avoids the issues of absolute errors in \Cref{sec:absolute.error.issue}.

\begin{itemize}
    \item [(i)] Condition (b) of \Cref{theo:relative.error} only requires the nuisance function estimators to be consistent and the product of the error of $\hat{e}(x)$ and $\tilde{\mu}_{1}(x)$, $\tilde{\mu}_{0}(x)$ to converge at $n^{-1/2}$.
    This condition of relative error is strictly weaker than the condition (b) of the absolute error in \cref{theo:absolute.error}.
    % \Cref{fig:inaccurate nuisance function estimator} demonstrates numerically that the  \cref{theo:absolute.error} is more susceptible to inaccurate nuisance function estimators.
    Particularly, when the treatment assignment is known, such as in randomized trials, then the validity of \Cref{theo:relative.error} only requires the consistency of $\tilde{\mu}_{1}(x)$, $\tilde{\mu}_{0}(x)$.


    In addition, condition (d) implies that the relative error estimator satisfies a global doubly robust property, meaning that if $\hat{e}(x) = e(x)$, then $\hat{\mu}_1(x)$ and $\hat{\mu}_0(x)$ can be arbitrary, even inconsistent, and the estimated relative error will still be unbiased. Similarly, the relative error remains unbiased if $\hat{\mu}_1(x)$ and $\hat{\mu}_0(x)$ are correct, regardless of the estimator $\hat{e}(x)$ used.
    This property does not hold for the absolute error estimator.

    
    \item [(ii)] Relative error estimators directly estimates and performs inference for the relative difference between two models' performance, rather than dealing with each model's error separately and then comparing them. 
    Therefore, unlike the absolute error estimation approach, there is no need to handle the dependency between two absolute error confidence intervals.
    
    \item [(iii)] Degenerate null. 
    Condition (c) of \Cref{theo:absolute.error} $\hat{\tau}(x) \neq \tau(x)$ is hard to validate or falsify, since $\tau(x)$ is not directly observed and needs to be estimated.
    In contrast, condition (c) of \Cref{theo:relative.error} $\hat{\tau}_1(x) \neq \hat{\tau}_2(x)$ can be verified straightfowardly, since $\hat{\tau}_1(x)$ and $\hat{\tau}_2(x)$ are provided functions and there is no extra estimation required.


    \item [(iv)] The relative error is more effective in identifying the better HTE estimators compared to the absolute error when $\hat{\tau}_1$, $\hat{\tau}_2$ are similar (see \Cref{fig:similar HTE estimator} for a numerical example).
    % To determine the better estimator based on the confidence intervals of absolute errors, we can conclude $\hat{\tau}_1$ is better if $\underline{\hat{\phi}}(\hat{\tau}_1; 1 - \alpha/2) > \overline{\hat{\phi}}(\hat{\tau}_2; 1 - \alpha/2)$.
    When $\hat{\tau}_1(X)$ and $\hat{\tau}_2(X)$ are similar, the confidence intervals for the absolute errors of $\hat{\tau}_1(X)$ and $\hat{\tau}_2(X)$ can still be wide, and the absolute error confidence intervals are too wide to be informative of which estimator is better.
    In contrast, the width of the confidence interval for $\hat{\delta}(\hat{\tau}_1, \hat{\tau}_2)$ is proportional to $\sqrt{\EE[(\hat{\tau}_1(X) - \hat{\tau}_2(X))^2]}$, which we prove in the appendix, and the confidence interval of the relative error can still be sufficiently narrow to reliably identify the more accurate estimator.   
\end{itemize}

\begin{figure}[h]
        \centering
        \begin{minipage}{0.5\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/similar_HTE_estimator_LASSO.pdf}
        \end{minipage}
        \caption{
        Relative error evaluation estimators are more effective in determining the better one of two similar HTE estimators.
        We consider two similar HTE estimators using LASSO for nuisance function estimation which only differ in the regularization hyperparameter. 
        We consider the IF \parencite{alaa2019validating}, EIF (our proposal) absolute, relative error confidence intervals.
        The $90\%$ absolute error confidence intervals of both IF and EIF are too wide to distinguish the two HTE estimators, while the confidence interval of the relative error estimator is significantly narrower and find the better estimator $\hat{\tau}_2$ (the true relative error is indicated in dark red and $\hat{\tau}_2$ has a smaller error).
        We remark that even if IF's relative error confidence interval stays above zero, it does not contain the true relative error.
        }
    \label{fig:similar HTE estimator}
\end{figure}


\subsection{Beyond difference in conditional means}\label{rmk:DINA}

For responses generated from an exponential family or the Cox model, such as when $ Y \mid X $ follows a Poisson distribution, the difference in the natural parameter functions between the treatment and the control has been proposed as the treatment effect estimand \parencite{gao2022DINA}.
The difference in natural parameter functions is better suited for modeling covariate dependence, as transforming to the natural parameter scale eliminates support constraints.
Let $\eta(\cdot)$ be the link function transforming the conditional mean function to the natural parameter scale, then the DINA estimand is 
\begin{align*}
    \tau(x) := \eta(E[Y(1) \mid X = x]) - \eta(E[Y(0) \mid X = x]).
\end{align*} 
Similarly we can define the absolute error~\eqref{defi:evaluation.error}, the relative error~\eqref{defi:evaluation.error.relative}, 
\begin{align*}
    &\hat{\phi}(\hat{\tau})
    := \frac{1}{n} \sum_{i=1}^n \left((\tilde{\eta}_1(X_i) - \tilde{\eta}_0(X_i)) - \hat{\tau}(X_i)\right)^2  \\
    &+ 2\left((\tilde{\eta}_1(X_i) - \tilde{\eta}_0(X_i)) - \hat{\tau}(X_i)\right) \cdot \left(\frac{W_i\tilde{\eta}'_1(X_i)(Y_i - \tilde{\mu}_1(X_i))}{\tilde{e}(X_i)} - \frac{(1-W_i)\tilde{\eta}'_0(X_i)(Y_i - \tilde{\mu}_0(X_i))}{1-\tilde{e}(X_i)} \right),
\end{align*}
and derive the efficient one-step correction estimators, \begin{align*}    
    &\hat{\phi}(\delta(\hat{\tau}_1, \hat{\tau}_2))
    := \frac{1}{n} \sum_{i=1}^n \hat{\tau}_1^2(X_i) - \hat{\tau}_2^2(X_i) - 2\left(\hat{\tau}_1(X_i) - \hat{\tau}_2(X_i)\right)\\
    & \cdot \left(\frac{W_i \tilde{\eta}'_1(X_i) (Y_i - \tilde{\mu}_1(X_i))}{\tilde{e}(X_i)} + \tilde{\eta}_1(X_i)- \frac{(1-W_i)\tilde{\eta}'_0(X_i)(Y_i - \tilde{\mu}_0(X_i))}{1-\tilde{e}(X_i)} - \tilde{\eta}_0(X_i)\right).
\end{align*}
Here $\tilde{\mu}_1(x)$, $\tilde{\mu}_0(x)$ are estimated nuisance mean functions, and $\tilde{\eta}_1(x)$, $\tilde{\eta}_0(x)$ are estimated natural parameter functions.
The estimators recover \eqref{eq:estimator.absolute.error} and \eqref{eq:estimator.relative.error} when the link function $\eta(\cdot)$ is identity. 
In the appendix, we present analogous results to \Cref{theo:absolute.error} and \Cref{theo:relative.error}.
The relative error of DINA maintains the advantages in \Cref{sec:relative.error.advantage}.



\section{Hypothetical real data analysis}\label{sec:simulation}

The ACIC 2016 competition dataset \parencite{dorie2019automated}, derived from the real-world Collaborative Perinatal Project \parencite{niswander1972women}, is used as a benchmark dataset for HTE estimation and related tasks. 
The dataset includes $55$ real variables of various types with natural associations. 
Treatment assignments and potential outcomes are generated using a variety of models. 
Therefore, the true $\mu_0(x)$, $\mu_1(x)$ and thus true $\tau(x)$ are known.
We select four representative scenarios varying two key factors: the model of the propensity score and the model of the group conditional functions. 
Specifically, the scenarios include: (a) linear $\mu_0(x)$, $\mu_1(x)$, linear $e(x)$; (b) nonlinear $\mu_0(x)$, $\mu_1(x)$, linear $e(x)$; (c) linear $\mu_0(x)$, $\mu_1(x)$, nonlinear $e(x)$; (d) nonlinear $\mu_0(x)$, $\mu_1(x)$, nonlinear $e(x)$.
Here, linear functions refer to a linear combination of covariates $x$, while nonlinear functions incorporate quadratic or higher-order terms of the covariates. 
For both linear and non-linear functions, the nuisance functions only depend on less than $20\%$ of the covariates.


The total sample size is $4802$. We use a randomly sampled $2802$ data points to obtain the HTE estimators for comparison, and the rest $2000$ data points to evaluate and compare the estimators' performance.


In terms of the HTE estimators for comparison, we consider two T-learners \parencite{Kunzel4156}.
The estimators first estimate $\mu_0(x)$, $\mu_1(x)$ separately and then take the difference $\hat{\mu}_1(x) - \hat{\mu}_0(x)$.
The key difference between two HTE estimators lies in the machine learning algorithms used to obtain $\hat{\mu}_0(x)$, $\hat{\mu}_1(x)$. 
\begin{itemize}
    \item [(i)] LASSO. This method uses LASSO\footnote{We use the R package \textsc{glmnet} for implementation. In particular, we use \textsc{cv.glmnet} to choose the regularization parameter.} to estimate $\mu_0(x)$ and $\mu_1(x)$.
    \item [(ii)] Boosting. This method uses gradient boosting\footnote{We use \textsc{xgboost} for implementation.} to estimate $\mu_0(x)$ and $\mu_1(x)$.
\end{itemize}
For settings (a) and (c), $\mu_0(x)$, $\mu_1(x)$ are linear and LASSO is well-specified. Therefore, the LASSO-based T-learner is expected to be more accurate. 
In contrast, for settings (b) and (d), $\mu_0(x)$, $\mu_1(x)$ are nonlinear, and the boosting-based T-learner should perform more favorably.


We implement five assessment methods: three based on absolute error estimation and two based on relative error estimation.
For absolute error estimators, we consider
\begin{itemize}
    \item One-step correction estimator based on our efficient influence function (EIF), as defined in \eqref{eq:estimator.absolute.error}.
    \item One-step correction estimator by \cite{alaa2019validating} (IF).
    \item Plug-in estimator (plug in): $\sum_{i=1}^n (\hat{\tau}(X_i) - \tilde{\tau}(X_i))^2 / n$, where $\tilde{\tau}(x)$ is the AIPW estimator of the HTE $\tau(x)$ obtained from the test dataset.
\end{itemize}
For relative error estimators, we consider
\begin{itemize}
    \item One-step correction estimator based on our efficient influence function (EIF) in \eqref{eq:estimator.relative.error}. 
    We note that the relative error estimator implied by the plug-in estimator of the absolute error is equivalent to EIF relative error estimator.
    Therefore, we merge the two methods in display.
    
    \item One-step correction estimator of the relative error based on the influence function in \cite{alaa2019validating}.
\end{itemize}
We use 2-fold cross-fitting.
All five methods share the same nuisance function estimators for fair comparison, which we specify case by case below.


To compute the coverage and average width of the confidence intervals ($90\%$ confidence level), and the probability of selecting the correct model (selection accuracy), for scenarios (a) through (d), we generate $100$ different realizations of $\mu_0(x)$, $\mu_1(x)$, and $e(x)$, adhering to the linear/nonlinear constraints aforementioned. For each realization of $\mu_0(x)$, $\mu_1(x)$, and $e(x)$, we further simulate $Y$ and $Z$ independently $100$ times to compute the frequency that the confidence intervals cover the true errors.
Similarly for other metrics above.


\subsection{Confidence interval coverage}\label{sec:coverage}

In \Cref{fig:simulation.coverage}, we examine the simplest scenario (a), where both the propensity score function $e(x)$ and the outcome functions $\mu_0(x)$ and $\mu_1(x)$ are linear. We compare the five evaluation methods equipped with three nuisance functions: (i) the true nuisance functions, (ii) a well-specified linear model, and (iii) an erroneous gradient boosting learner.


For relative errors, the EIF confidence interval is consistently valid, robust to the nuisance function estimator provided.
In contrast, the coverage of the relative error IF confidence interval fails to achieve the correct coverage.
For absolute errors, the coverage depends heavily on the accuracy of the nuisance learners. 
When the true nuisance estimators are used, the coverage of the EIF method for both the LASSO HTE estimator and the Boosting HTE estimator is close to the target level. However, as we shift to the nuisance estimator obtained using linear regression (which is well-specified but not as accurate), the coverage decreases, and it almost never covers the true value when the inaccurate gradient boosting with underfitting is used. 
The absolute error estimator IF consistently undercovers, even with true nuisance functions.
Additionally, as shown in the appendix, 
% \Cref{fig:simulation.width}
the width of the confidence intervals based on IF, for both relative and absolute error estimators, is significantly larger than those based on EIF, indicating less precision.


\subsection{Probability of selecting the winner}\label{sec:selection.accuracy}

We present the probability of selecting the correct model in \Cref{fig:ACIC.selection.accuracy}. The analysis covers settings (a) through (d), where in (a) and (c), the LASSO HTE estimator outperforms the Boosting HTE estimator by a moderate margin, whereas in (b) and (d), the Boosting HTE estimator significantly outperforms the LASSO HTE estimator.
For absolute error confidence intervals, we select the better estimator if the two confidence intervals at the $\alpha/2$ level do not overlap. Otherwise, no selection is made. 
In contrast, for relative error confidence intervals, we select the better estimator if the confidence interval for the relative error does not contain zero.


For the relative error-based methods, we observe that across the four scenarios, the EIF relative error method achieves the highest selection accuracy, followed by the IF relative error method. 
For the absolute error-based methods, the selection accuracy of the EIF absolute error method is lower in settings (a) and (c). This is because the improvement of the LASSO HTE estimator over the Boosting counterpart is relatively modest, and the gap between them is overshadowed by the wide confidence intervals, resulting in the methods being unable to confidently make a selection. 
In settings (b) and (d), the gap between the two HTE estimators is more pronounced, leading to a higher probability of correct selection. 
for the plug-in and IF absolute error methods, the confidence intervals are too wide, preventing any confident conclusions made across scenarios.


\begin{figure}[h]
        \centering
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_true_coverage_LASSO.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_true_coverage_Boosting.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_true_coverage_LASSO_V.S._Boosting.pdf}
        \end{minipage}       
        \\ \subcaption*{(i) True}        
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_linear_coverage_LASSO.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_linear_coverage_Boosting.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_linear_coverage_LASSO_V.S._Boosting.pdf}
        \end{minipage}        
        \\ \subcaption*{(ii) Linear regression} 
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_gradient_boosting_coverage_LASSO.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_gradient_boosting_coverage_Boosting.pdf}
        \end{minipage}
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/simulation_gradient_boosting_coverage_LASSO_V.S._Boosting.pdf}
        \end{minipage}        
        \subcaption*{(iii) Gradient boosting with underfitting}
        \caption{ 
        Coverage of the estimated absolute (LASSO, Boosting)/relative (LASSO V.S. Boosting) error's $90\%$ confidence intervals across three methods (plug in, IF, EIF) over three nuisance functions ((i) true, (ii) estimated by linear regression, (iii) estimated by gradient boosting with underfitting).
        }
    \label{fig:simulation.coverage}
\end{figure}


\begin{figure}[h]
        \centering
        \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_linear_propensity_linear_HTE_selection_accuracy_absolute_error.pdf}
        \end{minipage}
            \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_linear_propensity_linear_HTE_selection_accuracy_relative_error.pdf}
        \end{minipage}
        \subcaption*{(a) Linear $\mu_0(x)$, $\mu_1(x)$, linear $e(x)$} 
                \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_linear_propensity_nonlinear_HTE_selection_accuracy_absolute_error.pdf}
        \end{minipage}
            \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_linear_propensity_nonlinear_HTE_selection_accuracy_relative_error.pdf}
        \end{minipage}
        \subcaption*{(b) Nonlinear $\mu_0(x)$, $\mu_1(x)$, linear $e(x)$} 
                \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_nonlinear_propensity_linear_HTE_selection_accuracy_absolute_error.pdf}
        \end{minipage}
            \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_nonlinear_propensity_linear_HTE_selection_accuracy_relative_error.pdf}
        \end{minipage}
        \subcaption*{(c) Linear $\mu_0(x)$, $\mu_1(x)$, nonlinear $e(x)$} 
                \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_nonlinear_propensity_nonlinear_HTE_selection_accuracy_absolute_error.pdf}
        \end{minipage}
            \begin{minipage}{0.3\textwidth}
                \centering
                \includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width = \textwidth]{plot/ACIC_nonlinear_propensity_nonlinear_HTE_selection_accuracy_relative_error.pdf}
        \end{minipage}
        \subcaption*{(d) Nonlinear $\mu_0(x)$, $\mu_1(x)$, nonlinear $e(x)$} 
        \caption{
        The probability of correctly selecting the better HTE estimator by comparing absolute/relative error's confidence intervals across three methods (plug in, IF, EIF) over four scenarios of the ACIC competition data ((a) to (d)).   
        }
    \label{fig:ACIC.selection.accuracy}
\end{figure}



\section{Discussions}\label{sec:discussion}

The evaluation of HTE estimators typically involves additional estimation steps due to the inherent missingness of counterfactuals. 
We advocate that the comparison of HTE estimators should account for this uncertainty incurred in the test stage to determine the more accurate estimator with statistical confidence. 
We propose to achieve this goal by constructing confidence intervals for the relative error between two HTE estimators rather than their absolute errors.
Explicitly, we derive a one-step correction estimator based on the efficient influence function, and provide the asymptotically narrowest confidence interval of the relative error.
The relative error confidence interval is less sensitive to nuisance function estimation errors and is more powerful in identifying the better HTE estimator when the candidate estimators are similar.
Through empirical evaluation on the benchmark dataset from the 2016 ACIC challenge, we show that our relative error confidence interval achieves robust coverage regardless of the nuisance estimators used, and correctly determines the better HTE estimator with high probability.


We discuss several potential directions for future research. 
\begin{itemize}
    \item In this paper, we focus on evaluating the relative performance of provided HTE estimators. An intriguing direction for future exploration is how the relative error evaluation method can be integrated into the training process to produce a more accurate HTE estimator. Specifically, one potential approach is to use cross-validation in the HTE estimation, where the validation step is carried out using our proposed evaluation method. 

    \item In this paper, we investigate the comparison of HTE estimators' errors averaged across the entire population. However, it's important to note that some HTE estimator could be overall accurate but considerably less precise in underrepresented groups due to insufficient training data. 
    This presents a fairness concern, especially since HTE estimators could influence downstream policy decisions. To address this, it would be beneficial to extend the evaluation to errors averaged over certain subgroups.
    This could also provide insights for improving the HTE estimators within these subgroups. 

    
    \item In this paper, we bypass the degeneracy issue of the asymptotic distribution of the one-step correction estimator by shifting attention from the absolute estimand (absolute error) to its relative counterpart (relative error). It is of interest whether this approach could be generalized to address the degeneracy issue in other scenarios.
\end{itemize}

\printbibliography

\appendix
\include{appendix}


\end{document}
